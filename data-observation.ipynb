{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> l\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "Hit Enter to continue: \n",
      "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
      "                           and Basque Subset)\n",
      "  [ ] crubadan............ Crubadan Corpus\n",
      "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
      "  [ ] dolch............... Dolch Word List\n",
      "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
      "                           Corpus\n",
      "  [ ] floresta............ Portuguese Treebank\n",
      "  [ ] framenet_v15........ FrameNet 1.5\n",
      "  [ ] framenet_v17........ FrameNet 1.7\n",
      "  [ ] gazetteers.......... Gazeteer Lists\n",
      "  [ ] genesis............. Genesis Corpus\n",
      "  [ ] gutenberg........... Project Gutenberg Selections\n",
      "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
      "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
      "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
      "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
      "                           ChaSen format)\n",
      "  [ ] kimmo............... PC-KIMMO Data Files\n",
      "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
      "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
      "                           for parser comparison\n",
      "Hit Enter to continue: \n",
      "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
      "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
      "                           part-of-speech tags\n",
      "  [ ] machado............. Machado de Assis -- Obra Completa\n",
      "  [ ] masc_tagged......... MASC Tagged Corpus\n",
      "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
      "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
      "  [ ] moses_sample........ Moses Sample Models\n",
      "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
      "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
      "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
      "                           2015) subset of the Paraphrase Database.\n",
      "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
      "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
      "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
      "  [ ] nps_chat............ NPS Chat\n",
      "  [ ] omw................. Open Multilingual Wordnet\n",
      "  [ ] opinion_lexicon..... Opinion Lexicon\n",
      "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
      "  [ ] paradigms........... Paradigm Corpus\n",
      "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
      "                           Evaluation Shared Task\n",
      "Hit Enter to continue: \n",
      "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
      "                           character properties in Perl\n",
      "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
      "  [ ] pl196x.............. Polish language of the XX century sixties\n",
      "  [ ] porter_test......... Porter Stemmer Test Files\n",
      "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
      "  [ ] problem_reports..... Problem Report Corpus\n",
      "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
      "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
      "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
      "  [ ] pros_cons........... Pros and Cons\n",
      "  [ ] ptb................. Penn Treebank\n",
      "  [ ] punkt............... Punkt Tokenizer Models\n",
      "  [ ] qc.................. Experimental Data for Question Classification\n",
      "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
      "                           version\n",
      "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
      "                           Portuguesa)\n",
      "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
      "  [ ] sample_grammars..... Sample Grammars\n",
      "  [ ] semcor.............. SemCor 3.0\n",
      "Hit Enter to continue: \n",
      "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
      "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
      "  [ ] sentiwordnet........ SentiWordNet\n",
      "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
      "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
      "  [ ] smultron............ SMULTRON Corpus Sample\n",
      "  [ ] snowball_data....... Snowball Data\n",
      "  [ ] spanish_grammars.... Grammars for Spanish\n",
      "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
      "  [ ] stopwords........... Stopwords Corpus\n",
      "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
      "  [ ] swadesh............. Swadesh Wordlists\n",
      "  [ ] switchboard......... Switchboard Corpus Sample\n",
      "  [ ] tagsets............. Help on Tagsets\n",
      "  [ ] timit............... TIMIT Corpus Sample\n",
      "  [ ] toolbox............. Toolbox Sample Files\n",
      "  [ ] treebank............ Penn Treebank Sample\n",
      "  [ ] twitter_samples..... Twitter Samples\n",
      "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
      "                           (Unicode Version)\n",
      "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
      "Hit Enter to continue: \n",
      "  [ ] unicode_samples..... Unicode Samples\n",
      "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
      "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
      "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
      "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
      "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
      "  [ ] webtext............. Web Text Corpus\n",
      "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
      "  [ ] word2vec_sample..... Word2Vec Sample\n",
      "  [ ] wordnet............. WordNet\n",
      "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
      "  [ ] words............... Word Lists\n",
      "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
      "                           English Prose\n",
      "\n",
      "Collections:\n",
      "  [ ] all-corpora......... All the corpora\n",
      "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
      "                           branch\n",
      "  [ ] all................. All packages\n",
      "  [ ] book................ Everything used in the NLTK Book\n",
      "  [ ] popular............. Popular packages\n",
      "Hit Enter to continue: \n",
      "  [ ] tests............... Packages for running tests\n",
      "  [ ] third-party......... Third-party data packages\n",
      "\n",
      "([*] marks installed packages)\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> \n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jencmart/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This&i..s[an]example?{of}string.with.?punctuation!!!!'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_str = '0000T1111his42314123424&i4.4.s[an]example?{of}string.with.?punctuation!!!!' # Sample string\n",
    "delete_punct_translator = str.maketrans('', '', string.digits)\n",
    "input_str.translate(delete_punct_translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{48: None,\n",
       " 49: None,\n",
       " 50: None,\n",
       " 51: None,\n",
       " 52: None,\n",
       " 53: None,\n",
       " 54: None,\n",
       " 55: None,\n",
       " 56: None,\n",
       " 57: None}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = \"h3110 23 cat 444.4 rabbit 11 2 dog\"\n",
    "[s for s in ss.split() if s.isdigit()]\n",
    "\n",
    "remove_digits = ss.maketrans('', '', string.digits)\n",
    "remove_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_str = ''\n",
    "\n",
    "result = []\n",
    "max_len = len(my_str)\n",
    "i = 0\n",
    "while True :\n",
    "    if i >= max_len:\n",
    "        break\n",
    "        \n",
    "    if my_str[i].isdigit() :\n",
    "        num = ''\n",
    "        while i < max_len and my_str[i].isdigit():\n",
    "            num += my_str[i]\n",
    "            i += 1\n",
    "        if num != '':\n",
    "            result.append(num)\n",
    "    \n",
    "    # not the digit now\n",
    "    no_dig = ''\n",
    "    while i < max_len and not my_str[i].isdigit():\n",
    "            no_dig += my_str[i]\n",
    "            i += 1\n",
    "    if no_dig != '':\n",
    "        result.append(no_dig)\n",
    "            \n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'fasdf fdsfsd f                sfsd                     '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "tweet = \"I am tired! I like fruit...and milk\"\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "print(type(tweet.translate(translator)))\n",
    "\n",
    "\"fasdf fdsfsd f...!@#$%^&*/././sfsd.%^&$(^&(&*^/./.-+\\/.\".translate(translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction \n",
    "\n",
    "\n",
    "### 2. Goal\n",
    "\n",
    "# 3. Detailed specification\n",
    "\n",
    " A) Use a programming language of your choice and implement an experimental\n",
    "    retrieval system based on vector space model. For a given set of documents\n",
    "    and a set of topics, the system is expected to rank the documents according\n",
    "    to decreasing relevance to the topics. You are allowed to use standard\n",
    "    libraries. You are not allowed to use any IR-specific libraries/packages/\n",
    "    frameworks (e.g. trees and hashes are OK, inverted index is not).   \n",
    "\n",
    "    Expected usage:\n",
    "    ./run -q topics.xml -d documents.lst -r run -o sample.res ...\n",
    "\n",
    "    Where:\n",
    "      -q topics.xml -- a file including topics in the TREC format (see Sec 5.2)\n",
    "      -d document.lst -- a file including document filenames (see Sec 5.1)\n",
    "      -r run -- a label identifying the experiment (to be inserted in the\n",
    "         result file as \"run_id\", see Sec 5.3)\n",
    "      -o sample.res -- an output file (see Sec 5.3)\n",
    "      ... (additional parameters specific for your implementation)\n",
    "\n",
    "    You will have to deal with the following issues:\n",
    "     a) extraction of terms from the input data (data reading, tokenization,\n",
    "        punctuation reamoval, ...)\n",
    "     b) grouping terms based on equivalence classes (none, case folding,\n",
    "        lemmatization, stemming, number normalization, ...)\n",
    "     c) removing stopwords (none, frequency-based, POS-based, lexicon-based,\n",
    "        ...)\n",
    "     d) query construction (using title, desc, narr, ...)\n",
    "     e) term weighting (boolean, natural, logarithm, log average, augmented,\n",
    "        ...)\n",
    "     f) document frequency weighting (none, idf, probabilistic idf, ...)\n",
    "     g) vector normalization (none, cosine, pivoted, ...) \n",
    "     h) similarity measurement (cosine, Dice, Jaccard...)\n",
    "     i) relevance feedback (none, pseudo-relevance, ...)\n",
    "     j) query expansion (none, thesaurus-based, ...)\n",
    "     \n",
    " B) Set up a baseline system (denoted as run-0) specified as follows:\n",
    "\n",
    "    run-0 (baseline):\n",
    "    - terms: word forms\n",
    "    - lowercasing: no\n",
    "    - removing stopwords: no\n",
    "    - query construction: all word forms from \"title\"\n",
    "    - term weighting: natural\n",
    "    - document frequency weighting: none\n",
    "    - vector normalization: cosine\n",
    "    - similarity measurement: cosine\n",
    "    - relevance feedback: none\n",
    "    - query expansion: none\n",
    "    \n",
    " C) Use the provided Czech and English test collections and generate results\n",
    "    for the training and test topics for both the languages:\n",
    "\n",
    "    Example usage:\n",
    "    ./run -q topics-train_cs.xml -d documents_cs.lst -r run-0_cs -o run-0_train_cs.res\n",
    "\n",
    "    Include at most 1000 top-ranked documents for each topic.\n",
    "\n",
    "    You are exected to provide the following four files:\n",
    "    - run-0_train_cs.res\n",
    "    - run-0_test_cs.res\n",
    "    - run-0_train_en.res\n",
    "    - run-0_test_en.res\n",
    "    \n",
    "    The results for training topics can be evaluated using the trec_eval tool.\n",
    " \n",
    "    Example usage:\n",
    "    ./eval/trec_eval -M1000 qrels_train_cs.txt run-0_train_cs.res\n",
    "    \n",
    "    The evaluation methodology is described in Section 6.\n",
    "        \n",
    " D) Modify the baseline system by employing alternative/more advanced methods\n",
    "    for solving the issues and select the best combination (denoted as run-1)\n",
    "    which optimizes the system's performance on the set of training topics\n",
    "    (use Mean Average Precision as the main evaluation measure) for each of the\n",
    "    two languages.\n",
    "    \n",
    "    You are allowed to use any third-party text processing/annotation tool\n",
    "    (e.g. MorphoDiTa, available from http://ufal.mff.cuni.cz/morphodita, which\n",
    "    usefull for lemmatzation and available for both Czech and English). You may\n",
    "    use different approaches for Czech and for English (e.g. lemmatization for\n",
    "    Czech, stemming for English).\n",
    "\n",
    "    Justify your decisions by conducting comparative experiments. (For example,\n",
    "    if you decide to index lemmas instead of word forms, show that a system\n",
    "    based on forms performs worse on training topics. Or, if you decide to\n",
    "    exclude some stop words from the index, show that such a system performs\n",
    "    better than a system indexing all words. Or if you employ some query\n",
    "    expansion technique, show that it improves results for the trainin topics).\n",
    "\n",
    "    The only constraints are: \n",
    "    \n",
    "    i) the queries are constructed automatically\n",
    "    as words from the topic titles only (i.e. you cannot use the topic\n",
    "    descriptions and topic narratives to construct the queries)\n",
    "    \n",
    "    ii) retrieval\n",
    "    is completely automatic (no user intervention is allowed).\n",
    "    \n",
    "    run-1 (constrained):\n",
    "    - terms: ???\n",
    "    - lowercasing: ???\n",
    "    - removing stopwords: ???\n",
    "    - query construction: automatic from titles only\n",
    "    - term weighting: ???\n",
    "    - document frequency weighting: ???\n",
    "    - vector normalization: ???\n",
    "    - similarity measurement: ???\n",
    "    - relevance feedback: only pseudo-relevance-based techniques are allowed\n",
    "    - query expansion: ???\n",
    "\n",
    "    Generate result files of this system for the training topics and test\n",
    "    topics for both Czech and English. Include at most 1000 top-ranked\n",
    "    documents for each topic.\n",
    "\n",
    "    You are exected to provide the following four files:\n",
    "    - run-1_train_cs.res\n",
    "    - run-1_test_cs.res\n",
    "    - run-1_train_en.res\n",
    "    - run-1_test_en.res\n",
    "    \n",
    " E) Optionally, you can submit another system (denoted as run-2) with\n",
    "    absolutely no restrictions. You can use modified queries and use external\n",
    "    data resources (e.g. thesauri).\n",
    " \n",
    "    run-2 (unconstrained):\n",
    "    - terms: ???\n",
    "    - lowercasing: ???\n",
    "    - removing stopwords: ???\n",
    "    - query construction: ???\n",
    "    - term weighting: ???\n",
    "    - document frequency weighting: ???\n",
    "    - vector normalization: ???\n",
    "    - similarity measurement: ???\n",
    "    - relevance feedback: ???\n",
    "    - query expansion: ???\n",
    "    \n",
    "    Again, generate result files for the training topics and test topics.\n",
    "    Include at most 1000 top-ranked documents for each topic.\n",
    "\n",
    " F) Write a detailed report (in Czech or English) describing details of your\n",
    "    system (including building instructions, if needed), all the submitted runs\n",
    "    (including instructions how the result files were generated) all conducted\n",
    "    experiments and report their results on the training topics. Discuss the\n",
    "    results and findings. Compare the results and approaches for Czech and\n",
    "    English.\n",
    "    \n",
    "    For all experiments, report \"map\" (Mean Average Precision) as the main\n",
    "    evaluation measure nad \"P_10\" (Precision of the 10 first documents) as the\n",
    "    secodary measure. For run-0 and run-1 with the training topics (both Czech\n",
    "    and English) plot the Averaged 11-point averaged precision/recall graphs and\n",
    "    include them in the report. \n",
    "    \n",
    " G) Prepare a short presentation (a few slides, 5-10 minutes) summarizing your\n",
    "    approach, employed data structures, conducted experiments, results,\n",
    "    decisions, unsolved issues etc., to be presented to the lecturer and\n",
    "    your classmates during the practicals.  \n",
    "\n",
    "\n",
    "# 4. Package contents\n",
    "\n",
    " - documents_cs -- a directory containing 221 xml files with the total of 81735\n",
    "     documents in Czech (format description in Sec 5.1)\n",
    "\n",
    " - documents_en -- a directory containing 365 xml files with the total of 88110\n",
    "     documents in Czech (format description in Sec 5.1)\n",
    "   \n",
    " - documents_cs.lst -- a list of 221 filenames containing the Czech documents\n",
    " - documents_en.lst -- a list of 365 filenames containing the English documents\n",
    "\n",
    " - qrels-train_cs.xml -- manually assessed relevance judgements for the Czech\n",
    "     training topics (format description in Sec 5.4)\n",
    " - qrels-train_en.xml -- manually assessed relevance judgements for the English\n",
    "     training topics (format description in Sec 5.4)\n",
    " - qrels-test_cs.xml -- manually assessed relevance judgements for the Czech\n",
    "     test topics (not provided to students)\n",
    " - qrels-test_en.xml -- manually assessed relevance judgements for the English\n",
    "     test topics (not provided to students)\n",
    "\n",
    " - topics-train_cs.xml -- specification of the training topics in Czech\n",
    " - topics-train_en.xml -- specification of the training topics in English\n",
    " - topics-test_cs.xml -- specification of the test topics in Czech\n",
    " - topics-test_en.xml -- specification of the test topics in Czech\n",
    "\n",
    " - topics-test.lst  -- identifiers of the training topics\n",
    " - topics-train.lst -- identifiers of the test topics\n",
    "\n",
    " - sample-results.res -- example of result file (see Sec 5.3)\n",
    "\n",
    " - trec_eval-9.0.7.tar.gz -- the source code of the evaluation tool (see the\n",
    "   included README for building instructions)\n",
    "\n",
    " - README -- this file\n",
    "\n",
    "# 5. File formats\n",
    "\n",
    "### 5.1 Document format\n",
    "\n",
    "The document format uses a labeled bracketing, expressed in the style of SGML.\n",
    "The SGML DTD used for verification at TREC/CLEF is included in the archive. The\n",
    "philosophy in the formatting at NIST has been to preserve as much of the\n",
    "original structure as possible, but to provide enough consistency to allow\n",
    "simple decoding of the data.\n",
    "\n",
    "Every document is bracketed by <DOC> </DOC> tags and has a unique document\n",
    "number, bracketed by <DOCNO> </DOCNO> tags. The set of tags varies depending\n",
    "on the language (see the DTD for more details). The Czech documents typically\n",
    "contain the following tags (with corresponding end tags):\n",
    "\n",
    "```\n",
    "<DOC>\n",
    "<DOCID>\n",
    "<DOCNO>\n",
    "<DATE>\n",
    "<GEOGRAPHY>\n",
    "<TITLE>\n",
    "<HEADING>\n",
    "<TEXT>\n",
    "```\n",
    "\n",
    "The English tag set is much richer. Generally all the textual content (in any\n",
    "type of bracktets) can be indexed.\n",
    "\n",
    "### 5.2 Topic format\n",
    "\n",
    "Topic specification uses the following SGML markup:\n",
    "\n",
    "```\n",
    "<top lang=\"cs\">\n",
    "<num>\n",
    "...\n",
    "</num>\n",
    "<title>\n",
    "...\n",
    "</title>\n",
    "<desc>\n",
    "...\n",
    "</desc>\n",
    "<narr>\n",
    "...\n",
    "</narr>\n",
    "</top>\n",
    "```\n",
    "\n",
    "### 5.3 TREC result format \n",
    "\n",
    "The format of the system results (.res files) contains 5 tab-separated\n",
    "columns:\n",
    "\n",
    "  1. qid\n",
    "  2. iter\n",
    "  3. docno\n",
    "  4. rank\n",
    "  5. sim\n",
    "  6. run_id\n",
    "\n",
    "Example:\n",
    "  10.2452/401-AH 0 LN-20020201065 0 0 baseline\n",
    "  10.2452/401-AH 0 LN-20020102011 1 0 baseline\n",
    "\n",
    "The important fields matter are \"qid\" (query ID, a string), \"docno\" (document\n",
    "number, a string appearing in the DOCNO tags in the documents), \"rank\" (integer\n",
    "starting from 0), \"sim\" (similarity score, a float) and \"run_id\" (identifying\n",
    "the system/run name, must be same for one file). The \"sim\" field is ignore by\n",
    "the evaluation tool but you are required to fill it.  The \"iter\" (string) field\n",
    "is ignored and unimportant for this assignment.\n",
    "\n",
    "### 5.4 Query relevance format \n",
    "\n",
    "Relevance for each \"docno\" to \"qid\" is determined from train.qrels, which\n",
    "contains 4 columns:\n",
    "  1. qid\n",
    "  2. iter\n",
    "  3. docno\n",
    "  4. rel\n",
    "\n",
    "The \"qid\", \"iter\", and \"docno\" fields are as described above, and \"rel\" is\n",
    "a Boolean (0 or 1) indicating whether the document is a relevant response for\n",
    "the query (1) or not (1).\n",
    "\n",
    "Example:\n",
    " 10.2452/401-AH 0 LN-20020201065 1\n",
    " 10.2452/401-AH 0 LN-20020201066 0\n",
    "\n",
    " Document LN-20020201065 is relevant to topic 10.2452/401-AH.\n",
    " Document LN-20020201066 is not relevant to topic 10.2452/401-AH.\n",
    " \n",
    " \n",
    "### 6. Evaluation\n",
    "\n",
    "The evaluation tool is provided in the \"trec_eval-9.0.7.tar.gz\" package.\n",
    "Consult the \"README\" file inside the package for detailed instructions. \n",
    "\n",
    "Evaluation is performed by executing:\n",
    " ./eval/trec_eval -M1000 train-qrels.dat sample.res \n",
    "\n",
    " where\n",
    " -M1000 specifies the maximum number of documents per query (do not change\n",
    " this parameter).\n",
    " \n",
    "The tool outputs a summary of evaluation statistics:\n",
    "\n",
    "  runid -- run_id\n",
    "  num_q -- number of queries\n",
    "  num_ret -- number of returned documents\n",
    "  num_rel -- number of relevant documents\n",
    "  num_rel_ret -- number of returned relevant documents\n",
    "  map -- mean average precision (this is the main evaluation measure).\n",
    "  ...\n",
    "  iprec_at_recall_0.00 -- Interpolated Recall - Precision Averages at 0.00 recall\n",
    "  ...\n",
    "  P_10 -- Precision of the 10 first documents\n",
    "\n",
    "For details see  http://trec.nist.gov/pubs/trec15/appendices/CE.MEASURES06.pdf\n",
    "\n",
    "### 7. Submission\n",
    "\n",
    "Submission will be done by email. You will need a single (zip, tgz) package\n",
    "containing:\n",
    "  - the source code of your system and a \"README\" file with instructions how to\n",
    "    build your system (if needed) and how to run the experiments.\n",
    "  - The result files for training and test topics for at least run-0 and run-1\n",
    "    for Czech and for English.\n",
    "  - The \"report.pdf\" file with your written report.\n",
    "  - The \"slides.pdf\" file with a few slides you will use for presentation\n",
    "    during the practicals.\n",
    "\n",
    "The run-0 result files must be obtained by running somthing like this:\n",
    "\n",
    "  ./run -q topics-train_cs.xml -d documents_cs.lst -r run-0_cs -o run-0_train_cs.res\n",
    "  ./run -q topics-train_en.xml -d documents_en.lst -r run-0_en -o run-0_train_en.res\n",
    "\n",
    "which run the experiment and generates the training result files for Czech and\n",
    "English.\n",
    "\n",
    "  ./run -q topics-test_cs.xml  -d documents_cs.lst -r run-0_cs -o run-0_test_cs.res\n",
    "  ./run -q topics-test_en.xml  -d documents_en.lst -r run-0_en -o run-0_test_en.res\n",
    "\n",
    "\n",
    "run the experiment and generates the test result files for Czech and English.\n",
    "\n",
    "For the best constrained system (run-1) and the best unconstrained system\n",
    "(run-2) optionally, provide instructions how to execute the experiments and\n",
    "obtain the results. Describe the systems/runs in detail in the report and\n",
    "summarize in the presentation. \n",
    "\n",
    "Always include at most 1000 top ranked documents for each query in each run\n",
    "(trec_eval will be executed  with parameter -M1000).\n",
    "\n",
    "### 8. Notes\n",
    "\n",
    "### 9. Grading\n",
    "\n",
    "\n",
    "### 10. Plagiarism and joined work\n",
    "\n",
    "### 11. Terms of use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OK --- IMPLEMENTATION HERE ---\n",
    "# 1.  First I explore the data a bit\n",
    "* I want to understand the data format\n",
    "* I think that some XML tags may be misleading\n",
    "* So for each xml tag I measure num. of occurences in whole dataset and num. of documents where it occured\n",
    "* Moreover I also sample some of the text inside the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def read_documents(lst, dir_name):  \n",
    "    l = {}\n",
    "\n",
    "    with open(lst) as f:\n",
    "        files = f.readlines()\n",
    "    # you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "    files = [x.strip() for x in files] \n",
    "\n",
    "    errors = {}\n",
    "\n",
    "    for file_name in files:\n",
    "        file = os.path.join(dir_name, file_name) # folder+'/'+file_name\n",
    "        try:\n",
    "            with open (file, \"r\") as myfile:\n",
    "                d=myfile.read()\n",
    "            with open ('extra.xml', \"r\") as myfile:\n",
    "                magic=myfile.read()\n",
    "            d = d.split(\"\\n\", 2)[2]\n",
    "            \n",
    "            # en la020120\n",
    "            d = d.replace(\"^N\", \"^M\")\n",
    "            # en la020707\n",
    "            d = d.replace(\"</LATIMES2002></LATIMES2002>\", \"</LATIMES2002>\")\n",
    "           \n",
    "            # cs ln020225.xml \n",
    "            d = d.replace(\"< >\", \"\")\n",
    "            d = d.replace('<IMG SRC=\" adresa, na které je soubor uložen \">', '')\n",
    "            d = d.replace('<A NAME=\" jméno kotvy \">', '')\n",
    "\n",
    "            # root = ET.parse(file).getroot()\n",
    "            root = ET.fromstring(magic+d)\n",
    "        except Exception as e:\n",
    "            errors[file_name] = e\n",
    "            with open('tmp', \"w\") as myfile:\n",
    "                myfile.write(magic+d)\n",
    "            continue\n",
    "\n",
    "     \n",
    "        \n",
    "        for child in root:\n",
    "#             print (child.tag) # DOC \n",
    "            \n",
    "            docID = child.find('DOCID')\n",
    "            \n",
    "            for c in child:\n",
    "               \n",
    "                if(c.text and len(c.text) > 0):\n",
    "#                     if(c.tag == \"ID\"):\n",
    "#                         print(c.text)\n",
    "                    \n",
    "                    if c.tag in l:\n",
    "                        \n",
    "                        if(l[c.tag][3] != docID):\n",
    "                            l[c.tag][3] = docID\n",
    "                            l[c.tag][4] +=1\n",
    "                            \n",
    "                        l[c.tag][0] += 1\n",
    "                        l[c.tag][1] += len(c.text)\n",
    "                        if(len(l[c.tag][2]) < 3 and random.random() < 0.1):\n",
    "                            if(len(c.text) > 60):\n",
    "                                 l[c.tag][2].append(c.text[:50])\n",
    "                            else:\n",
    "                                 l[c.tag][2].append(c.text[:50])\n",
    "\n",
    "                            \n",
    "\n",
    "                    else:\n",
    "                        l[c.tag] = [1, len(c.text) , [], docID, 1]\n",
    "                        \n",
    "    return (l, errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'la020120.xml': ParseError('not well-formed (invalid token): line 47474, column 4')}\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "import os\n",
    "import random \n",
    "res, errors = read_documents('documents_en.lst', 'documents_en')\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TE', [1200018, 1125733, 197.53695111239998])\n",
      "('LD', [245266, 159881, 218.36250438299643])\n",
      "('PT', [200232, 112470, 5.655859203324144])\n",
      "('ED', [175527, 87761, 7.342841841995818])\n",
      "('CP', [171164, 134904, 61.765429646420976])\n",
      "('PN', [168887, 81550, 1.0005861907666072])\n",
      "('PP', [168878, 81541, 3.301383247077772])\n",
      "('DC', [137239, 96640, 14.569087504280853])\n",
      "('DF', [137228, 96633, 14.568907220100854])\n",
      "('DD', [87766, 0, 23.26705102203587])\n",
      "('DOCNO', [87766, 0, 9.0])\n",
      "('DOCID', [87766, 0, 9.0])\n",
      "('PD', [87766, 0, 7.999954424264522])\n",
      "('SN', [87766, 0, 6.857496069092815])\n",
      "('PY', [87766, 0, 4.0])\n",
      "('DK', [87766, 0, 2.089920926098945])\n",
      "('PG', [87753, 0, 1.299214841657835])\n",
      "('EI', [87472, 0, 7.051513627217853])\n",
      "('CF', [87111, 0, 12.440311786111971])\n",
      "('HD', [86614, 9, 35.89936961692105])\n",
      "('WD', [85569, 0, 3.1528123502670358])\n",
      "('IN', [85569, 0, 1.7421729832065351])\n",
      "('TM', [83905, 30259, 7.820654311423634])\n",
      "('SL', [79862, 0, 11.521862713180235])\n",
      "('PR', [79856, 0, 1.0027299138449208])\n",
      "('DP', [79641, 0, 7.714719805125501])\n",
      "('IS', [78089, 0, 1.0])\n",
      "('HI', [69311, 0, 123.38451328072023])\n",
      "('BD', [59460, 0, 22.657332660612177])\n",
      "('FN', [55532, 21117, 15.979417272923719])\n",
      "('AU', [50023, 531, 14.706135177818204])\n",
      "('PH', [40137, 34, 12.004783616114807])\n",
      "('KH', [37022, 21, 16.255253632974988])\n",
      "('DH', [30518, 0, 109.37178058850515])\n",
      "('DL', [17514, 0, 11.907217083476077])\n",
      "('UP', [9721, 0, 8.0])\n",
      "('CX', [6333, 4567, 134.394441812727])\n",
      "('AN', [4109, 273, 15.899732294962279])\n",
      "('SM', [3446, 0, 39.31456761462565])\n",
      "('LA', [2085, 0, 7.106954436450839])\n",
      "('BR', [2085, 0, 5.504076738609113])\n",
      "('CB', [1790, 0, 28.96312849162011])\n",
      "('SI', [191, 3, 14.575916230366492])\n",
      "('SE', [159, 0, 63.069182389937104])\n",
      "('CO', [82, 0, 13.926829268292684])\n",
      "('PF', [38, 0, 1.0])\n",
      "('CN', [9, 0, 1.0])\n",
      "('CR', [7, 0, 29.142857142857142])\n",
      "('ID', [2, 0, 591.0])\n",
      "('JP', [2, 0, 4.5])\n",
      "('NO', [1, 0, 31.0])\n"
     ]
    }
   ],
   "source": [
    "# ('SI', [191, 14.575916230366492]) - citation ?\n",
    "\n",
    "for k, v in res.items():\n",
    "    l[k] = [v[0], v[0] - v[4], v[1] / v[0]] # v4 == num of doccuments\n",
    "sorted_x = sorted(l.items(), key=operator.itemgetter(1), reverse=True)        \n",
    "for k in sorted_x:\n",
    "    print(k)    \n",
    "# total_num_occurences\n",
    "# total_num_occurences - unique_occurences\n",
    "# average len of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "* Most of the tags are unique per document \n",
    "* Those which are not usually containts some important chunks of the text\n",
    "* Some tags seems to be missleading ( although we would be able to delete them as the stopwords later, I'll just skip them now..\n",
    "* I only counted those tags, which contained some text, maybe there are some tags which contain nothig across all documents - we won't care about them either\n",
    "* Let's output some sample text from the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TE', [1200018, 74285, 197, ['   At the Hollywood Reporter, editor Anita Busch, ', '   In November, as Richard Riordan announced his c', '   The final novella, \"The Trope Teacher,\" is narr']])\n",
      "('LD', [245266, 85385, 218, ['Los Angeles Commission on Assaults Against Women n', '   A Safer Nation', 'Re \"More Fitting Name for Swing Voters: Airheads,\"']])\n",
      "('PT', [200232, 87762, 5, ['Southern California Living', 'ME', 'ME']])\n",
      "('ED', [175527, 87766, 7, ['LA', 'Home Edition', 'Home Edition']])\n",
      "('CP', [171164, 36260, 61, ['ID NUMBER: 20020101g10d0rke', 'ID NUMBER: 20020101go617oke', 'No element type Friends of the Sea Lion Marine Mam']])\n",
      "('PN', [168887, 87337, 1, ['5', 'E', 'E']])\n",
      "('PP', [168878, 87337, 3, ['5-3', 'E-1', 'B-12']])\n",
      "('DC', [137239, 40599, 14, ['BIOTECHNOLOGY', 'FOOD AND DRUG ADMINISTRATION (U.S.)', 'OCCUPATIONAL TRAINING']])\n",
      "('DF', [137228, 40595, 14, ['RETAIL SALES', 'FOOD AND DRUG ADMINISTRATION (U.S.)', 'WEB SITES']])\n",
      "('DD', [87766, 87766, 23, ['Tuesday January 01, 2002', 'Tuesday January 01, 2002', 'Tuesday January 01, 2002']])\n",
      "('DOCID', [87766, 87766, 9, ['000000016', '000000020', '000000037']])\n",
      "('DOCNO', [87766, 87766, 9, ['000000015', '000000020', '000000023']])\n",
      "('PD', [87766, 87766, 7, ['20020101', '20020101', '20020101']])\n",
      "('SN', [87766, 87766, 6, ['TK69KO6', 'TK69KPB', 'SR69OML']])\n",
      "('PY', [87766, 87766, 4, ['2002', '2002', '2002']])\n",
      "('DK', [87766, 87766, 2, ['Features Desk', 'Features Desk', 'FI']])\n",
      "('PG', [87753, 87753, 1, ['2', '8', '2']])\n",
      "('EI', [87472, 87472, 7, ['cmagdaleno', 'cmagdaleno', 'cmagdaleno']])\n",
      "('CF', [87111, 87111, 12, ['Business Desk', 'Business Desk', 'Business Desk']])\n",
      "('HD', [86614, 86605, 35, ['INVOLVEMENT OPPORTUNITIES', 'California Classroom', 'SEC Plans Fraud Suit Against Aura']])\n",
      "('WD', [85569, 85569, 3, ['1767', '900', '1397']])\n",
      "('IN', [85569, 85569, 1, ['10', '21', '18']])\n",
      "('TM', [83905, 53646, 7, ['LE', 'WR', 'BR']])\n",
      "('SL', [79862, 79862, 11, ['fi-real1', 'fi-tax1', 'fi-airfees1']])\n",
      "('PR', [79856, 79856, 1, ['0', '0', '0']])\n",
      "('DP', [79641, 79641, 7, ['Editorials', 'Business', 'Business']])\n",
      "('IS', [78089, 78089, 1, ['N', 'N', 'N']])\n",
      "('HI', [69311, 69311, 123, ['PubDate:01-01-02;Zone:LA;Ed:1;Section:SoCal\\nLiving', 'PubDate:01-01-02Zone:LAEd:1Section:SoCal\\nLivingPag', 'PubDate:01-01-02;Zone:LA;Ed:1;Section:SoCal\\nLiving']])\n",
      "('BD', [59460, 59460, 22, ['SPECIAL TO THE TIMES', 'From Reuters', 'From Bloomberg News']])\n",
      "('FN', [55532, 34415, 15, ['20020101go617oke', '20020101goyu63ke', '20020101go6132ke']])\n",
      "('AU', [50023, 49492, 14, ['BRAD BERTON', '\\\\f7', 'BILL SHAIKIN']])\n",
      "('PH', [40137, 40103, 12, [\"The Kids' Reading Room\", \"The Kids' Reading Room\", 'Pro Basketball']])\n",
      "('KH', [37022, 37001, 16, ['Reading by 9', 'Reading by 9', 'IN BRIEF /  ECONOMY']])\n",
      "('DH', [30518, 30518, 109, ['Courts: Judge issues a temporary restraining order', 'College football: Moving to repair damage caused b', 'S. America: Party leaders work to rescue Argentina']])\n",
      "('DL', [17514, 17514, 11, ['WASHINGTON', 'MEXICO CITY', 'TEMPE, Ariz.\\n']])\n",
      "('UP', [9721, 9721, 8, ['20020101', '20020101', '20020101']])\n",
      "('CX', [6333, 1766, 134, ['FOR THE RECORD', 'FOR THE RECORD', 'For the Record']])\n",
      "('AN', [4109, 3836, 15, ['20020102O569TN0X', '20020102hme0002', '20020104M36BG44X']])\n",
      "('SM', [3446, 3446, 39, ['Southern California Living review 1/1/02. Bernadet', 'Question Corner', 'these are letters that were trimmed from the dec. ']])\n",
      "('LA', [2085, 2085, 7, ['E368VEI', '14045', 'E368V10']])\n",
      "('BR', [2085, 2085, 5, ['170', '312', '98560']])\n",
      "('CB', [1790, 1790, 28, ['SEE CORRECTION APPENDED\\n..CB:', 'SEE CORRECTION APPENDED\\n..CB:', 'SEE CORRECTION APPENDED\\n..CB:']])\n",
      "('SI', [191, 188, 14, ['\\\\o7 Larry Stewart', '\\\\o7 Mike Penner\\\\f7', 'Larry Stewart']])\n",
      "('SE', [159, 159, 63, ['Second of three articles\\n', 'Last of three articles\\n', 'INSIDE THE IOC :This is the last in a four-part se']])\n",
      "('CO', [82, 82, 13, ['T.J. SIMERS', 'POINTS WEST\\n', 'Bill Plaschke\\n']])\n",
      "('PF', [38, 38, 1, ['W', 'W', 'W']])\n",
      "('CN', [9, 9, 1, ['1']])\n",
      "('CR', [7, 7, 29, ['*Bridgestone Firestone inc;*Ford Motor Co', '*Daimlerchrysler;*automobile industry;*automobiles', '*MATTEL INC']])\n",
      "('ID', [2, 2, 591, ['<CorrectionBanner>\\n<OriginalUid>\\n<PubDate>\\n<DayDat']])\n",
      "('JP', [2, 2, 4, []])\n",
      "('NO', [1, 1, 31, []])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "for k, v in res.items():\n",
    "    l[k] = [v[0], v[4], math.floor(v[1] / v[0]), v[2]] # v0 - total, v4 - unique, ,v2 - sample\n",
    "sorted_x = sorted(l.items(), key=operator.itemgetter(1), reverse=True)        \n",
    "for k in sorted_x:\n",
    "    print(k)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ok so for now I'll use only those tags that **seems** to carry some usefull information\n",
    "* Use those which occured in more than 25% of the documents\n",
    "* do not use followig (not occurred in 25% )\n",
    "```\n",
    "('DL', [17514, 17514, 11, ['WASHINGTON', 'MEXICO CITY', 'TEMPE, Ariz.\\n']])\n",
    "('UP', [9721, 9721, 8, ['20020101', '20020101', '20020101']])\n",
    "('CX', [6333, 1766, 134, ['FOR THE RECORD', 'FOR THE RECORD', 'For the Record']])\n",
    "('AN', [4109, 3836, 15, ['20020102O569TN0X', '20020102hme0002', '20020104M36BG44X']])\n",
    "('SM', [3446, 3446, 39, ['Southern California Living review 1/1/02. Bernadet', 'Question Corner', 'these are letters that were trimmed from the dec. ']])\n",
    "('LA', [2085, 2085, 7, ['E368VEI', '14045', 'E368V10']])\n",
    "('BR', [2085, 2085, 5, ['170', '312', '98560']])\n",
    "('CB', [1790, 1790, 28, ['SEE CORRECTION APPENDED\\n..CB:', 'SEE CORRECTION APPENDED\\n..CB:', 'SEE CORRECTION APPENDED\\n..CB:']])\n",
    "('SI', [191, 188, 14, ['\\\\o7 Larry Stewart', '\\\\o7 Mike Penner\\\\f7', 'Larry Stewart']])\n",
    "('SE', [159, 159, 63, ['Second of three articles\\n', 'Last of three articles\\n', 'INSIDE THE IOC :This is the last in a four-part se']])\n",
    "('CO', [82, 82, 13, ['T.J. SIMERS', 'POINTS WEST\\n', 'Bill Plaschke\\n']])\n",
    "('PF', [38, 38, 1, ['W', 'W', 'W']])\n",
    "('CN', [9, 9, 1, ['1']])\n",
    "('CR', [7, 7, 29, ['*Bridgestone Firestone inc;*Ford Motor Co', '*Daimlerchrysler;*automobile industry;*automobiles', '*MATTEL INC']])\n",
    "('ID', [2, 2, 591, ['<CorrectionBanner>\\n<OriginalUid>\\n<PubDate>\\n<DayDat']])\n",
    "('JP', [2, 2, 4, []])\n",
    "('NO', [1, 1, 31, []])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the rest skip the:\n",
    "\n",
    "> examples of the tags use this format:\n",
    "\n",
    "> ```(tag, total_occcur, unique_occcur, avg_tex_len, [sample_1, sample_2, sample_3])```\n",
    "\n",
    "* **FN** .. some strange hash / id\n",
    "```\n",
    "('FN', [55532, 34415, 15, ['20020101go617oke', '20020101goyu63ke', '20020101go6132ke']])\n",
    "```\n",
    "* **PN**, **PP** .. some identificators ? \n",
    "```\n",
    "('PN', [168887, 87337, 1, ['5', 'E', 'E']])\n",
    "('PP', [168878, 87337, 3, ['5-3', 'E-1', 'B-12']])\n",
    "```\n",
    "\n",
    "* **PD**, **SN** .. also some identificators ?\n",
    "\n",
    "```\n",
    "('PD', [87766, 87766, 7, ['20020101', '20020101', '20020101']])\n",
    "('SN', [87766, 87766, 6, ['TK69KO6', 'TK69KPB', 'SR69OML']])\n",
    "```\n",
    "\n",
    "* **CP**, **IS**, **PR**, **PG**, **HI**, **DD**, **EI**, **PY**, **SL**\n",
    "\n",
    "```\n",
    "('CP', [171164, 36260, 61, ['ID NUMBER: 20020101g10d0rke', 'ID NUMBER: 20020101go617oke', 'No element type Friends of the Sea Lion Marine Mam']])\n",
    "('IS', [78089, 78089, 1, ['N', 'N', 'N']])\n",
    "('PR', [79856, 79856, 1, ['0', '0', '0']])\n",
    "('PG', [87753, 87753, 1, ['2', '8', '2']])\n",
    "('HI', [69311, 69311, 123, ['PubDate:01-01-02;Zone:LA;Ed:1;Section:SoCal\\nLiving', 'PubDate:01-01-02Zone:LAEd:1Section:SoCal\\nLivingPag', 'PubDate:01-01-02;Zone:LA;Ed:1;Section:SoCal\\nLiving']])\n",
    "('DD', [87766, 87766, 23, ['Tuesday January 01, 2002', 'Tuesday January 01, 2002', 'Tuesday January 01, 2002']])\n",
    "('EI', [87472, 87472, 7, ['cmagdaleno', 'cmagdaleno', 'cmagdaleno']])\n",
    "('PY', [87766, 87766, 4, ['2002', '2002', '2002']])\n",
    "('SL', [79862, 79862, 11, ['fi-real1', 'fi-tax1', 'fi-airfees1']])\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### so we use this for now:\n",
    "\n",
    "```\n",
    "('TE', [1200018, 74285, 197, ['Hollywood Reporter,...,  In November, as Richard R...', ' The final novella...']])\n",
    "('LD', [245266, 85385, 218, ['Los Angeles Commission on Assaults...', '   A Safer Nation', 'Re \"More Fitti...\"']])\n",
    "('PT', [200232, 87762, 5, ['Southern California Living', 'ME', 'ME']])\n",
    "('ED', [175527, 87766, 7, ['LA', 'Home Edition', 'Home Edition']])\n",
    "('DC', [137239, 40599, 14, ['BIOTECHNOLOGY', 'FOOD AND DRUG ADMINISTRATION (U.S.)', 'OCCUPATIONAL TRAINING']])\n",
    "('DF', [137228, 40595, 14, ['RETAIL SALES', 'FOOD AND DRUG ADMINISTRATION (U.S.)', 'WEB SITES']])\n",
    "('DK', [87766, 87766, 2, ['Features Desk', 'Features Desk', 'FI']])\n",
    "('CF', [87111, 87111, 12, ['Business Desk', 'Business Desk', 'Business Desk']])\n",
    "('HD', [86614, 86605, 35, ['INVOLVEMENT OPPORTUNITIES', 'California Classroom', 'SEC Plans Fraud Suit Against Aura']])\n",
    "('WD', [85569, 85569, 3, ['1767', '900', '1397']])\n",
    "('IN', [85569, 85569, 1, ['10', '21', '18']])\n",
    "('TM', [83905, 53646, 7, ['LE', 'WR', 'BR']])\n",
    "('DP', [79641, 79641, 7, ['Editorials', 'Business', 'Business']])\n",
    "('BD', [59460, 59460, 22, ['SPECIAL TO THE TIMES', 'From Reuters', 'From Bloomberg News']])\n",
    "('AU', [50023, 49492, 14, ['BRAD BERTON', '\\\\f7', 'BILL SHAIKIN']])\n",
    "('PH', [40137, 40103, 12, [\"The Kids' Reading Room\", \"The Kids' Reading Room\", 'Pro Basketball']])\n",
    "('KH', [37022, 37001, 16, ['Reading by 9', 'Reading by 9', 'IN BRIEF /  ECONOMY']])\n",
    "('DH', [30518, 30518, 109, ['Courts: Judge issues a temporary restraining order', 'College football: Moving to repair damage caused b', 'S. America: Party leaders work to rescue Argentina']])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EXPLORE CS VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2, errors2 = read_documents('documents_cs.lst', 'documents_cs')\n",
    "for k, v in errors2.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TEXT', [116552, 1304.1138204406616, 81735])\n",
      "('DOCID', [81735, 14.0, 81735])\n",
      "('DOCNO', [81735, 14.0, 81735])\n",
      "('DATE', [81735, 8.0, 81735])\n",
      "('TITLE', [68895, 63.088743740474634, 68895])\n",
      "('HEADING', [48732, 27.284433226627268, 25836])\n",
      "('GEOGRAPHY', [30247, 6.353357357754488, 30114])\n"
     ]
    }
   ],
   "source": [
    "lcs = {}\n",
    "for k, v in l2.items():\n",
    "    lcs[k] = [v[0], v[1] / v[0], v[4]]\n",
    "    \n",
    "sorted2 = sorted(lcs.items(), key=operator.itemgetter(1), reverse=True)        \n",
    "for k in sorted2:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For the CZECH version we have tags that overall seems to be in plausible form, I won't do any chage here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# IMPLEMENTATION OF THE  >> SPIMI << ALGORITHM\n",
    "### (Because why not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': '/home/jencmart/matfyz/sem-1/bio/hw-1/hw1',\n",
      " 'include': '/home/jencmart/matfyz/sem-1/bio/hw-1/hw1/include/python3.7m',\n",
      " 'platinclude': '/home/jencmart/matfyz/sem-1/bio/hw-1/hw1/include/python3.7m',\n",
      " 'platlib': '/home/jencmart/matfyz/sem-1/bio/hw-1/hw1/lib/python3.7/site-packages',\n",
      " 'platstdlib': '/home/jencmart/matfyz/sem-1/bio/hw-1/hw1/lib/python3.7',\n",
      " 'purelib': '/home/jencmart/matfyz/sem-1/bio/hw-1/hw1/lib/python3.7/site-packages',\n",
      " 'scripts': '/home/jencmart/matfyz/sem-1/bio/hw-1/hw1/bin',\n",
      " 'stdlib': '/home/jencmart/matfyz/sem-1/bio/hw-1/hw1/lib/python3.7'}\n"
     ]
    }
   ],
   "source": [
    "from sysconfig import get_paths\n",
    "from pprint import pprint\n",
    "\n",
    "info = get_paths()  # a dictionary of key-paths\n",
    "\n",
    "# pretty print it for now\n",
    "pprint(info)\n",
    "# {'data': '/usr/local',\n",
    "#  'include': '/usr/local/include/python2.7',\n",
    "#  'platinclude': '/usr/local/include/python2.7',\n",
    "#  'platlib': '/usr/local/lib/python2.7/dist-packages',\n",
    "#  'platstdlib': '/usr/lib/python2.7',\n",
    "#  'purelib': '/usr/local/lib/python2.7/dist-packages',\n",
    "#  'scripts': '/usr/local/bin',\n",
    "#  'stdlib': '/usr/lib/python2.7'}\n",
    "# You could also use the -m switch with sysconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need some sort of reusable token stream\n",
    "# This stream will produce list of words\n",
    "# we need some tokenization function\n",
    "\n",
    "\n",
    "class TokenStream:\n",
    "    def __init__(self, file_list, base_dir, header = None):\n",
    "        self.file_list = file_list\n",
    "        self.base_dir = base_dir\n",
    "        self.header = header\n",
    "    \n",
    "    def "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dictionary: dict loaded\n",
      "Loading tagger: tagger loaded\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from ufal.morphodita import *\n",
    "\n",
    "def encode_entities(text):\n",
    "    return text.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;').replace('\"', '&quot;')\n",
    "\n",
    "# In Python2, wrap sys.stdin and sys.stdout to work with unicode.\n",
    "if sys.version_info[0] < 3:\n",
    "    import codecs\n",
    "    import locale\n",
    "    encoding = locale.getpreferredencoding()\n",
    "    sys.stdin = codecs.getreader(encoding)(sys.stdin)\n",
    "    sys.stdout = codecs.getwriter(encoding)(sys.stdout)\n",
    "\n",
    "if len(sys.argv) == 1:\n",
    "    sys.stderr.write('Usage: %s tagger_file\\n' % sys.argv[0])\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "\n",
    "# not_eof = True\n",
    "# while not_eof:\n",
    "#     text = 'Rusové obsadili opuštěnou americkou základnu v Sýrii. Přiletěli bitevními vrtulníky'\n",
    "\n",
    "#   # Read block\n",
    "#     while True:\n",
    "#         line = sys.stdin.readline()\n",
    "#         not_eof = bool(line)\n",
    "#         if not not_eof: break\n",
    "#         line = line.rstrip('\\r\\n')\n",
    "#         text += line\n",
    "#         text += '\\n';\n",
    "#         if not line: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ------------------------------------- \n",
      "televize 0\n",
      "odvysílat 1\n",
      "záběr 2\n",
      ", 3\n",
      "na 4\n",
      "jenž 5\n",
      "ozbrojení 6\n",
      "vojenský 7\n",
      "policista 8\n",
      "přilétat 9\n",
      "na 10\n",
      "základna 11\n",
      "v 12\n",
      "provincie 13\n",
      "Halab 14\n",
      "nedaleko 15\n",
      "hranice 16\n",
      "s 17\n",
      "Turecko 18\n",
      "a 19\n",
      "začínat 20\n",
      "on 21\n",
      "zajišťovat 22\n",
      ". 23\n",
      "\n",
      " ------------------------------------- \n",
      "podle 24\n",
      "Zvezda 25\n",
      "mít 26\n",
      "komplex 27\n",
      "sloužit 28\n",
      "jako 29\n",
      "distribuční 30\n",
      "centrum 31\n",
      "humanitární 32\n",
      "pomoc 33\n",
      "pro 34\n",
      "místní 35\n",
      "obyvatelstvo 36\n",
      ". 37\n",
      "\n",
      " ------------------------------------- \n",
      "ovládat 38\n",
      "on 39\n",
      "jednotka 40\n",
      "syrský 41\n",
      "vláda 42\n",
      ", 43\n",
      "který 44\n",
      "být 45\n",
      "spojenec 46\n",
      "Moskva 47\n",
      ". 48\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "text = 'Televize odvysílala záběry, na nichž ozbrojení vojenští policisté přilétají na základnu v provincii Halab nedaleko hranice s Tureckem a začínají ji zajišťovat. Podle Zvezdy má komplex sloužit jako distribuční centrum humanitární pomoci pro místní obyvatelstvo. Ovládají ho jednotky syrské vlády, která je spojencem Moskvy.'\n",
    "\n",
    "\n",
    "  # Tag\n",
    "tokenizer.setText(text)\n",
    "t = 0\n",
    "tmp = 0\n",
    "while tokenizer.nextSentence(forms, tokens):\n",
    "    tagger.tag(forms, lemmas)\n",
    "    sys.stdout.write('\\n ------------------------------------- \\n')\n",
    "    \n",
    "    for i in range(len(lemmas)):\n",
    "    \n",
    "        \n",
    "        lemma = lemmas[i]\n",
    "        token = tokens[i]\n",
    "        print(morpho.rawLemma(lemma.lemma), tmp)\n",
    "        tmp+=1\n",
    "#         sys.stdout.write('%s%s<token lemma=\"%s\" tag=\"%s\">%s</token>%s' % (\n",
    "#         encode_entities(text[t : token.start]),\n",
    "#         \"<sentence>\" if i == 0 else \"\",\n",
    "#         encode_entities(lemma.lemma),\n",
    "#         encode_entities(lemma.tag),\n",
    "#         encode_entities(text[token.start : token.start + token.length]),\n",
    "#         \"</sentence>\" if i + 1 == len(lemmas) else \"\",\n",
    "#         ))\n",
    "        \n",
    "        t = token.start + token.length\n",
    "#         sys.stdout.write('\\n\\n')\n",
    "        \n",
    "#     sys.stdout.write(encode_entities(text[t : ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TOKEN STREAM IMPLEMENTATION (not thread safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from ufal.morphodita import *\n",
    "\n",
    "import queue\n",
    "import sys\n",
    "import os\n",
    "\n",
    "class TokenStream:\n",
    "    def __init__(self, file_list, base_dir, extra_header, dict_file, tagger_file, excluded_xml_tags):  \n",
    "        \n",
    "        self._prepared = None\n",
    "        \n",
    "        # load the tagger and dictionary\n",
    "        self._morpho = self._loadDictionary(dict_file)\n",
    "        self._tagger = self._loadTagger(tagger_file)\n",
    "        \n",
    "        # create tokenizer\n",
    "        self._forms = Forms()\n",
    "        self._lemmas = TaggedLemmas()\n",
    "        self._tokens = TokenRanges()\n",
    "        self._tokenizer = self._tagger.newTokenizer() # tokenizer based on tagger\n",
    "        if self._tokenizer is None:\n",
    "            sys.stderr.write(\"Error: No tokenizer is defined for the supplied model!\")\n",
    "            exit(1)\n",
    "        \n",
    "        # load file names\n",
    "        self._file_queue = self._readFileNames(file_list)        \n",
    "        self._base_dir = base_dir\n",
    "        self._extra_header = extra_header\n",
    "        self._excluded_xml_tags = excluded_xml_tags\n",
    "        \n",
    "        # queue for storing the documents\n",
    "        self._doc_queue = queue.Queue() \n",
    "        \n",
    "        # for info about current document\n",
    "        self._doc_no = 0\n",
    "        self._pos_in_doc = 0\n",
    "        self._curr_sentence_len = 0\n",
    "        self._word_in_sentence = 0\n",
    "        \n",
    "        # num of documents\n",
    "        self._cnt_documents = 0\n",
    "        \n",
    "        # for ability to tokenize one string\n",
    "        self._signleString = False\n",
    "    \n",
    "    def _readFileNames(self, file_list):\n",
    "        with open(file_list) as f:\n",
    "            file_names = f.readlines()\n",
    "            file_names = [x.strip() for x in file_names] \n",
    "            \n",
    "            q = queue.Queue()\n",
    "            for i in file_names:\n",
    "                q.put(i)\n",
    "            return q\n",
    "        \n",
    "    def _loadDictionary(self, file):\n",
    "        sys.stderr.write('Loading dictionary: ')\n",
    "        morpho = Morpho.load(file)\n",
    "        if not morpho:\n",
    "            sys.stderr.write(\"Error: Cannot load dictionary from file '%s'\\n\" % sys.argv[1])\n",
    "            sys.exit(1)\n",
    "        sys.stderr.write('success\\n')\n",
    "        exit(1)\n",
    "        return morpho\n",
    "\n",
    "    def _loadTagger(self, file):\n",
    "        sys.stderr.write('Loading tagger: ')\n",
    "        tagger = Tagger.load(file)\n",
    "        if not tagger:\n",
    "            sys.stderr.write(\"Error: Cannot load tagger from file '%s'\\n\" % sys.argv[1])\n",
    "            sys.exit(1)\n",
    "        sys.stderr.write('success\\n')\n",
    "        exit(1)\n",
    "        return tagger\n",
    "    \n",
    "    \n",
    "    def _read_xml_file(self, file):\n",
    "        try:\n",
    "            with open (file, \"r\") as myfile:\n",
    "                d=myfile.read()\n",
    "                \n",
    "            # if we have custom header, replace it with the original header\n",
    "            if(self._extra_header):\n",
    "                with open ('extra.xml', \"r\") as myfile:\n",
    "                    extra_head=myfile.read()\n",
    "                d = d.split(\"\\n\", 2)[2]\n",
    "                d = extra_head+d\n",
    "               \n",
    "            \n",
    "            # --- SOME HARD CODED FIXES ---------------------------------------------\n",
    "            # en la020120\n",
    "            d = d.replace(\"^N\", \"^M\")\n",
    "            # en la020707\n",
    "            d = d.replace(\"</LATIMES2002></LATIMES2002>\", \"</LATIMES2002>\")\n",
    "           \n",
    "            # cs ln020225.xml \n",
    "            d = d.replace(\"< >\", \"\")\n",
    "            d = d.replace('<IMG SRC=\" adresa, na které je soubor uložen \">', '')\n",
    "            d = d.replace('<A NAME=\" jméno kotvy \">', '')\n",
    "            # -----------------------------------------------------------------------\n",
    "\n",
    "                \n",
    "            root = ET.fromstring(d)\n",
    "            return root\n",
    "        except Exception as e:\n",
    "            raise e;\n",
    "    \n",
    "    def _nextFile(self):\n",
    "        \n",
    "        while(not self._file_queue.empty()): # while because some files may be broken\n",
    "            file_name = self._file_queue.get()\n",
    "            file = os.path.join(self._base_dir, file_name)\n",
    "            try:\n",
    "                root = self._read_xml_file(file)\n",
    "            except Exception as e:\n",
    "                file_name += '  ['\n",
    "                file_name += e\n",
    "                file_name += ']'\n",
    "                sys.stderr.write(\"Error: reading file: '%s'\\n\" % file_name)\n",
    "                continue\n",
    "                \n",
    "            self._xml_to_document_list(root)\n",
    "            return True\n",
    "        \n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def _xml_to_document_list(self, root):\n",
    "        \n",
    "        # for all documents in the file\n",
    "        for child in root:\n",
    "            docID = child.find('DOCNO').text\n",
    "            \n",
    "            doc_as_string = ''\n",
    "            # for tags in this document\n",
    "            for c in child:\n",
    "                # we have some data and it is not in the excluded tag\n",
    "                if(c.text and len(c.text) > 0 and not c.tag in self._excluded_xml_tags):\n",
    "                    doc_as_string += ' '\n",
    "                    doc_as_string += c.text\n",
    "                    \n",
    "            # fill the queue with documents from current file\n",
    "            self._doc_queue.put([docID, doc_as_string])\n",
    "            \n",
    "    def _nextDocument(self):\n",
    "        if(not self._doc_queue.empty()): # if we have some documents in memory, just load it\n",
    "            docID, doc_data = self._doc_queue.get()\n",
    "            self._doc_no = docID\n",
    "            self._pos_in_doc = 0\n",
    "            self._cnt_documents +=1\n",
    "            self._tokenizer.setText(doc_data)\n",
    "            return True\n",
    "        \n",
    "        elif(self._nextFile()):\n",
    "            return self._nextDocument()\n",
    "        \n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "        \n",
    "    def _nextSentence(self):\n",
    "        # try to read next sentence\n",
    "        if self._tokenizer.nextSentence(self._forms, self._tokens):\n",
    "            self._tagger.tag(self._forms, self._lemmas)\n",
    "            self._word_in_sentence = 0\n",
    "            self._curr_sentence_len = len(self._lemmas)\n",
    "            return True\n",
    "        \n",
    "        else:\n",
    "            if self._signleString:\n",
    "                self._signleString = False\n",
    "                return False\n",
    "            \n",
    "            # otherwise try to read next document \n",
    "            if(self._nextDocument()):\n",
    "#                 print('loading next document')\n",
    "                return self._nextSentence()\n",
    "            \n",
    "            # no other files, we are at the end my friend\n",
    "            return False\n",
    "    \n",
    "    def hasNext(self):\n",
    "        if(self._prepared):\n",
    "            return True\n",
    "        else:\n",
    "            x = self.nextLemma()\n",
    "            if(x):\n",
    "                self._prepared = x\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "    def tokenizeString(self, string):\n",
    "        self._tokenizer.setText(string)\n",
    "        self._signleString = True\n",
    "   \n",
    "        \n",
    "    def numOfDocuments(self):\n",
    "        return self._cnt_documents\n",
    "    \n",
    "    def nextLemma(self):\n",
    "        if(self._prepared):\n",
    "            tmp = self._prepared\n",
    "            self._prepared = None\n",
    "            return tmp\n",
    "        \n",
    "        # check if we still have lemmas in sentece, if not, load new sentence\n",
    "        if(self._word_in_sentence >= self._curr_sentence_len):\n",
    "#             print('loading next sentence')\n",
    "            # try to load next sentence\n",
    "            if(not self._nextSentence()):\n",
    "                return False\n",
    "            \n",
    "        lemma = self._lemmas[self._word_in_sentence]\n",
    "        token = self._tokens[self._word_in_sentence]\n",
    "        self._pos_in_doc+=1\n",
    "        self._word_in_sentence+=1\n",
    "        return (self._doc_no, self._morpho.rawLemma(lemma.lemma), self._pos_in_doc-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPIMI IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dictionary: success\n",
      "Loading tagger: success\n"
     ]
    }
   ],
   "source": [
    "tokens_cs =   ['DOCID', 'DOCNO', 'TITLE',  'HEADING' 'TEXT', 'DATE', 'GEOGRAPHY']\n",
    "excluded_cs = ['DOCNO', 'DOCID']\n",
    "\n",
    "tokStr = TokenStream(file_list='./documents_cs.lst', \n",
    "                  base_dir='./documents_cs/', \n",
    "                  extra_header='./extra.xml',\n",
    "                  dict_file='./lang_models/czech-morfflex-pdt-161115/czech-morfflex-161115-pos_only.dict',\n",
    "                  tagger_file='./lang_models/czech-morfflex-pdt-161115/czech-morfflex-pdt-161115-pos_only.tagger',\n",
    "                  excluded_xml_tags=excluded_cs)\n",
    "\n",
    "# todo\n",
    "def enoughMemory(mem):\n",
    "    siz = 4+4+5 # docId, docPos, term\n",
    "    mem += 1\n",
    "    if(mem==30000000): ## 100k 1M , 10M\n",
    "        return 0\n",
    "    return mem\n",
    "\n",
    "\n",
    "# todo\n",
    "def mergeBlocks(outDir, base_prefix, base_suffix, cntBlocks):\n",
    "    # todo\n",
    "    print('merging blocks: ', cntBlocks)\n",
    "\n",
    "    \n",
    "def writePositional(file, dictionary):\n",
    "    with open (file, \"w\") as myfile:\n",
    "        \n",
    "        for i in sorted (dictionary):  \n",
    "\n",
    "            w = i + ' '\n",
    "            myfile.write(w)\n",
    "\n",
    "            for key, val in dictionary[i].items():\n",
    "                myfile.write(\"%s [\" % key )\n",
    "                for item in val:\n",
    "                    myfile.write(\"%s \" % item)\n",
    "                myfile.write('] ')\n",
    "            myfile.write('\\n')\n",
    "\n",
    "def writeVector(file, dictionary, cntDoc, cntWords):\n",
    "    with open (file, \"w\") as myfile:\n",
    "        myfile.write( cntDoc + ' ' + cntWords +  '\\n')\n",
    "        for i in sorted (dictionary):  \n",
    "\n",
    "            w = i + ' '\n",
    "            myfile.write(w) # write the key -- \"pes\"\n",
    "            myfile.write(str(dictionary[i][0])) # TF\n",
    "            \n",
    "            for key, val in dictionary[i][1].items():\n",
    "                myfile.write(\" %s %s\" % (key, val) ) # DOCNO DF\n",
    "            myfile.write('\\n')\n",
    "\n",
    "def insertPositional(dictionary, tokenStream):\n",
    "     # insert into the dictionary\n",
    "    docID, token, position = tokenStream.nextLemma()\n",
    "    \n",
    "    ## if already in dictionary\n",
    "    if(token in dictionary):\n",
    "\n",
    "        ## moreover if doc exist\n",
    "        if (docID in dictionary[token] ):\n",
    "            dictionary[token][docID] . append(position)\n",
    "        else:\n",
    "            dictionary[token][docID] = [position]\n",
    "\n",
    "\n",
    "    else:\n",
    "        dictionary[token] =  {docID : [position] }\n",
    "    return dictionary \n",
    "def writeDocDicToFile(file, docDic):\n",
    "    with open (file, \"w\") as myfile:\n",
    "        for k, v in docDic.items():\n",
    "            myfile.write(k + '\\n')         \n",
    "            \n",
    "def insertForVectorModel(docDic, dictionary, tokenStream):\n",
    "     # insert into the dictionary\n",
    "    docID, token, position = tokenStream.nextLemma()\n",
    "\n",
    "    # assign ID to the DOCID\n",
    "    if(docID in docDic):\n",
    "        docID = docDic[docID]\n",
    "    else:\n",
    "        x = len(docDic) # od 0\n",
    "        docDic[docID] = x\n",
    "        docID = x\n",
    "        \n",
    "    ## if already in dictionary\n",
    "    if(token in dictionary):\n",
    "\n",
    "        ## moreover if doc exist\n",
    "        if (docID in dictionary[token][1] ):\n",
    "            dictionary[token][1][docID] += 1 # more in this doc \n",
    "            dictionary[token][0] += 1 # more overall\n",
    "        else:\n",
    "            dictionary[token][1][docID] = 1 # first in this doc\n",
    "            dictionary[token][0] += 1 # more over all\n",
    "\n",
    "    else:\n",
    "        dictionary[token] = [1, {docID : 1 }] # first over TF , first here DF\n",
    "    return dictionary \n",
    "\n",
    "def spimi(tokenStream, insertIntoDictionary, writeDicToFile):\n",
    "    cntBlocks = 0\n",
    "    mem = 0\n",
    "    docDic = {}\n",
    "    \n",
    "    # while we have some stuff in tokenstream\n",
    "    while(tokenStream.hasNext()):\n",
    "        print(\"working on block: \", cntBlocks)\n",
    "        # prepare file \n",
    "        out_dir = './out'\n",
    "        base_prefix = 'part_'\n",
    "        base_suffix = '.txt'\n",
    "        \n",
    "        # and dictionary \n",
    "        dictionary = {}\n",
    "        \n",
    "        while(tokenStream.hasNext()):\n",
    "            mem = enoughMemory(mem)\n",
    "            if(mem == 0):\n",
    "                break\n",
    "                \n",
    "            dictionary = insertIntoDictionary(docDic, dictionary, tokenStream)    \n",
    "           \n",
    "                \n",
    "        file = os.path.join(out_dir, base_prefix + str(cntBlocks) + base_suffix)\n",
    "        # todo -- for now disabled == one giant doc\n",
    "#         writeDicToFile(file, dictionary)\n",
    "#         cntBlocks +=1\n",
    "#         if(cntBlocks == 10):\n",
    "#             break\n",
    "            \n",
    "    # TODO - write last block (after the break)\n",
    "    file = os.path.join(out_dir, base_prefix + str(cntBlocks) + base_suffix)\n",
    "    cntDoc = str(tokenStream.numOfDocuments())\n",
    "    writeDicToFile(file, dictionary, cntDoc, str(len(dictionary)))\n",
    "\n",
    "    # write docDic\n",
    "    file = os.path.join(out_dir, 'docDic' + base_suffix)\n",
    "    writeDocDicToFile(file, docDic)\n",
    "    # merge blocks\n",
    "    # mergeBlocks(out_dir, base_prefix, base_suffix, cntBlocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on block:  0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-2b3b30b528db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspimi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokStr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minsertForVectorModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriteVector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# about 20M - 30M words (lemmas)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-d24d79adeda9>\u001b[0m in \u001b[0;36mspimi\u001b[0;34m(tokenStream, insertIntoDictionary, writeDicToFile)\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minsertIntoDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocDic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-d24d79adeda9>\u001b[0m in \u001b[0;36minsertForVectorModel\u001b[0;34m(docDic, dictionary, tokenStream)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m## moreover if doc exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdocID\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdocID\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# more in this doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# more overall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "spimi(tokStr, insertForVectorModel, writeVector)\n",
    "# about 20M - 30M words (lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Vector Model from Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def vectorModelToMatrix(file):\n",
    "    with open(file) as fp:\n",
    "        line = fp.readline()\n",
    "        r = line.split(\" \")\n",
    "        N = int(r[0]) # docs\n",
    "        v = int(r[1]) # words\n",
    "        \n",
    "        # alloc whole matrix - oh my god !\n",
    "        # matrix = np.zeros((v, N), dtype=\"float32\") # words x docs\n",
    "        token_id = 0\n",
    "        matrix = {}\n",
    "        \n",
    "        dict_token__id_tf = {}\n",
    "        \n",
    "        # start with the first word\n",
    "        line = fp.readline()\n",
    "        while line:\n",
    "            r = line.split(\" \")\n",
    "            \n",
    "            \n",
    "            term  = r[0] \n",
    "            tf = int(r[1]) \n",
    "            log_tf = math.log(tf, 10)\n",
    "            dict_token__id_tf[term] = [token_id, tf] # id for word, TF\n",
    "            for i in range(2,len(r), 2):\n",
    "                docID = int(r[i])\n",
    "                df = 0.0\n",
    "                df += int(r[i+1])\n",
    "                \n",
    "                # tf-idf\n",
    "                tf_idf = (1 + log_tf) * math.log(N/df)\n",
    "                if docID in matrix:\n",
    "                    matrix[docID][token_id] = tf_idf # unique\n",
    "                else:\n",
    "                    matrix[docID] = {token_id : tf_idf}\n",
    "                \n",
    "            line = fp.readline()\n",
    "            token_id +=1\n",
    "            \n",
    "    return (matrix, dict_token__id_tf) # transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we need to tread query as document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryAsVector(query, tokenStream, vectorModel, wordDictionary):\n",
    "    tokenStream.tokenizeString(query)\n",
    "    \n",
    "    # load query to the tokenizer\n",
    "    \n",
    "    # and dictionary \n",
    "    queryDict = {}\n",
    "    \n",
    "    norm_vector = 0.0\n",
    "    \n",
    "    # get tokens of the query\n",
    "    while(tokenStream.hasNext()):\n",
    "#         print('OK')\n",
    "        docID, token, position = tokenStream.nextLemma()\n",
    "\n",
    "        ## if already in dictionary\n",
    "        if(token in queryDict):\n",
    "            queryDict[token][1] = 1 # doc freq\n",
    "            queryDict[token][0] += 1 # term freq\n",
    "\n",
    "        else:\n",
    "            if(token in wordDictionary):\n",
    "                tf = wordDictionary[token][1] + 1 # +1 for this occur\n",
    "            else:\n",
    "                # if token not in wordDict.. skip it\n",
    "                tf = 1\n",
    "                continue # q_i * 0\n",
    "                \n",
    "            queryDict[token] = [tf, 1] # TF , DF\n",
    "     \n",
    "    vector = {}\n",
    "    \n",
    "    # calculate tf-idf   \n",
    "    N = len(vectorModel) # {docID : {word : tf_idf, ... ,} }\n",
    "    for token, v in queryDict.items():\n",
    "        tf = v[0]\n",
    "        df = v[1]\n",
    "        log_tf = math.log(tf, 10) \n",
    "#         print(log_tf)\n",
    "        tf_idf = (1 + log_tf) * math.log(N/df)\n",
    "        \n",
    "        norm_vector += tf_idf * tf_idf\n",
    "        \n",
    "        if(tf > 1) : # it was in word dict ( no need for dot otherwise)\n",
    "            cntWord = wordDictionary[token][0] # id\n",
    "            vector[cntWord] = tf_idf\n",
    "    \n",
    "    # maybe unecessary\n",
    "    vect2 = {}\n",
    "    for i in sorted (vector) : \n",
    "        vect2[i] = vector[i]\n",
    "        \n",
    "    return vect2, math.sqrt(norm_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47712125471966244\n",
      "0.30102999566398114\n",
      "0.47712125471966244\n",
      "0.47712125471966244\n",
      "0.30102999566398114\n",
      "0.30102999566398114\n",
      "0.30102999566398114\n",
      "0.30102999566398114\n",
      "0.30102999566398114\n",
      "0.7781512503836435\n",
      "0.30102999566398114\n",
      "0.30102999566398114\n",
      "0.30102999566398114\n",
      "0.30102999566398114\n",
      "0.30102999566398114\n",
      "0.47712125471966244\n",
      "0.30102999566398114\n",
      "0.30102999566398114\n",
      "0.30102999566398114\n",
      "0.30102999566398114\n",
      "0.30102999566398114\n",
      "0.30102999566398114\n",
      "0.30102999566398114\n",
      "0.30102999566398114\n",
      "0.30102999566398114\n",
      "0.30102999566398114\n",
      "0.30102999566398114\n",
      "0.47712125471966244\n",
      "0.30102999566398114\n",
      "0.30102999566398114\n",
      "0.30102999566398114\n",
      "0.30102999566398114\n",
      "0.30102999566398114\n",
      "0.30102999566398114\n",
      "0.30102999566398114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2: 14.716259387058418}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newQuery = 'Praha - Prahu a okolí včera večer zasáhla ledová kalamita, jakou hlavní město dlouho nepamatuje. Auta nemohla jezdit, chodcům ujížděly nohy, ruzyňské letiště nepřijímalo letadla, protože by na ledě nezastavila, a dálnice v okolí metropole byly zablokované.'\n",
    "\n",
    "vector, norm = queryAsVector(newQuery, tokStr, matrix, dictionary)\n",
    "norm\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSINE SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosSim(d1, d2, norm1 = None):\n",
    "    \n",
    "    dot_d1_d2 = dot(d1,d2)\n",
    "    if norm1:\n",
    "        d1_norm = norm1\n",
    "    else:\n",
    "        d1_norm = norm(d1)\n",
    "    d2_norm = norm(d2)\n",
    "    \n",
    "    p = (d1_norm*d2_norm)\n",
    "    if(d1_norm == 0 and d2_norm != 0):\n",
    "        print(\"d1_norm == 0\")\n",
    "        return 0.0\n",
    "    if(d1_norm != 0 and d2_norm == 0):\n",
    "        print(\"d2_norm == 0\")\n",
    "        return 0.0\n",
    "    if(p==0):\n",
    "        print(\" both == 0\")\n",
    "        return 0.0\n",
    "#     print(\"ok\")\n",
    "    return dot_d1_d2/p\n",
    "\n",
    "def norm(d1):\n",
    "    sq_sum = 0.0\n",
    "    \n",
    "    for k, v in d1.items():\n",
    "        sq_sum += v*v\n",
    "    return math.sqrt(sq_sum)\n",
    "\n",
    "def dot(d1, d2):\n",
    "    d1 = [(k, v) for k, v in d1.items()] \n",
    "    d2 = [(k, v) for k, v in d2.items()] \n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "    d1_len = len(d1)\n",
    "    d2_len = len(d2)\n",
    "    \n",
    "    dot = 0.0\n",
    "    while True:\n",
    "        if i >= d1_len or j >= d2_len:\n",
    "            break\n",
    "            \n",
    "        if(d1[i][0] == d2[j][0] ):\n",
    "            dot += d1[i][1] * d2[j][1]\n",
    "            i+=1\n",
    "            j+=1\n",
    "            continue\n",
    "            \n",
    "        if(d1[i][0] < d2[j][0]):\n",
    "            while i < d1_len and  d1[i][0] < d2[j][0] :\n",
    "                i+=1\n",
    "            continue\n",
    "            \n",
    "        if(d2[j][0] < d1[i][0] ):\n",
    "            while j < d2_len and d2[j][0] < d1[i][0]:\n",
    "                j+=1\n",
    "            continue\n",
    "    return dot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity list for all docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similList(query, tokenStream,  vectorModel, wordDictionary):\n",
    "    \n",
    "    vector, norm = queryAsVector(query, tokenStream, vectorModel, wordDictionary)\n",
    "    # vector = {wordID : tf_idf}\n",
    "    \n",
    "    \n",
    "    max_sim = -1.0\n",
    "    docID = -1.0\n",
    "    \n",
    "    res = {}\n",
    "    for k, v in vectorModel.items(): # k = docID , v = {wordID : tf_idf, ... ,}\n",
    "        sim = cosSim(vector, v, norm)\n",
    "        res[k] = sim\n",
    "        if (sim > max_sim):\n",
    "            max_sim = sim\n",
    "            docID = k\n",
    "    return (res, max_sim, docID)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix, dictionary = vectorModelToMatrix('out/part_0.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9168187144038387\n",
      "32290\n"
     ]
    }
   ],
   "source": [
    "# test query\n",
    "# MF-20021227001\n",
    "newQuery = 'KSČM měla zisk z pašování zbraní Komunisté se zapletli s firmou, která nelegálně vyvážela do KLDR Praha - Komunistická strana se prostřednictvím svých lídrů v uplynulých letech spojila s pašeráky zbraní a vytěžila z toho mimo jiné dvoumilionovou půjčku pro stranický list Haló noviny. Předseda poslanců KSČM Vojtěch Filip pracoval v první polovině 90. let pro českobudějovickou firmu KAMO, jejíž ruský zakladatel Alexandr Kaljandra byl odsouzen za nelegální vývoz vojenských automobilů Tatra. Vozy skončily v Číně a Severní Koreji. Českobudějovický zastupitel Stanislav Křída (KSČM) ve firmě KAMO figuroval jako prokurista. Tato společnost navíc platila cestu místopředsedy strany Vlastimila Balína do Korejské lidově demokratické republiky. MF DNES to zjistila během pátrání po majetkových poměrech komunistických politiků. Informace vycházejí z účetnictví firmy KAMO, ze svědectví bývalé Kaljandrovy manželky a zčásti i z vlastních slov komunistů. \"Do firmy jsem přišel, protože Kaljandra byl můj švagr. Pak jsem tam přivedl i kolegu Filipa,\" přiznal Křída. Vedení KSČM se ke spojení svých lidí s firmou nedokázalo vyjádřit. \"Nemyslím si o tom nic, protože o tom v podstatě nic nevím,\" prohlásil místopředseda strany Václav Exner, který má už sedmým rokem na starosti stranické financování. Na všechny otázky o spolupráci jeho kolegů s firmou KAMO a její půjčce na rozjezd Haló novin Exner odpovídal: \"Možné to je.\" Místopředseda Balín, za jehož cestu do Koreje firma KAMO komunistům zaplatila 68 tisíc korun, přitom tvrdí, že o tyto věci se stará Exner. \"Kdo to platil, to já nevím, to se musíte zeptat kolegy Exnera.\" Ani poslanec Filip spolupráci s firmou KAMO nepopřel, avšak mluví o ní jako o bezvýznamné záležitosti.'\n",
    "\n",
    "lst, max_sim, docID = similList(newQuery, tokStr, matrix, dictionary)    \n",
    "print(max_sim)\n",
    "print(docID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id_map = []\n",
    "with open('out/docDic.txt' , 'r') as myfile:\n",
    "    line = myfile.readline()\n",
    "    while(line):\n",
    "        doc_id_map.append(line[:-1])\n",
    "        line = myfile.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MF-20020418001'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_id_map[32290]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-7dd3504c366f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
